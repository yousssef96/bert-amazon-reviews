{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a63c87-fdf2-4875-85fe-135bf837f7f4",
   "metadata": {},
   "source": [
    "# Amazon Reviews for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5b1422f-b541-49f0-92a5-2ac70ce91cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, \n",
    "from lightning.pytorch.callbacks import Callback, ModelCheckpoint\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af7f3490-3b69-4322-9be4-ef04a085efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets\n",
    "train_dataset = pd.read_csv('train.ft.txt', sep=\"\\t\", header=None, names=['text'])\n",
    "test_dataset = pd.read_csv('test.ft.txt', sep=\"\\t\", header=None, names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50cfe901-d79f-46e0-ba54-cfda66c0dc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2 Stuning even for the non-gamer: Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2 The best soundtrack ever to anythin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__2 Amazing!: This soundtrack is my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2 Excellent Soundtrack: I truly like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2 Remember, Pull Your Jaw Off The Flo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  __label__2 Stuning even for the non-gamer: Thi...\n",
       "1  __label__2 The best soundtrack ever to anythin...\n",
       "2  __label__2 Amazing!: This soundtrack is my fav...\n",
       "3  __label__2 Excellent Soundtrack: I truly like ...\n",
       "4  __label__2 Remember, Pull Your Jaw Off The Flo..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "092d293a-f116-4c57-bba9-c5ca73078d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'label' column in the training and test datasets with value 0 for '__label__1' and 1 for ' __label__2'\n",
    "train_dataset['label'] = train_dataset['text'].apply(lambda x : 0 if '__label__1' in x else 1)\n",
    "test_dataset['label'] = test_dataset['text'].apply(lambda x : 0 if '__label__1' in x else 1)\n",
    "\n",
    "# Clean the training and test text columns by removing the labels from the beginning\n",
    "train_dataset['text'] = train_dataset['text'].str.replace(r'__label__[12]', '', regex=True).str.strip()\n",
    "test_dataset['text'] = test_dataset['text'].str.replace(r'__label__[12]', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a13a3bc-3d55-40d3-a7f0-ebd1943416ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Stuning even for the non-gamer: This sound tra...      1\n",
       "1  The best soundtrack ever to anything.: I'm rea...      1\n",
       "2  Amazing!: This soundtrack is my favorite music...      1\n",
       "3  Excellent Soundtrack: I truly like this soundt...      1\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d407173c-b668-4d23-8416-395426ea580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1800000, 1800000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Class distribution:\")\n",
    "np.bincount(train_dataset[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd381fc2-3680-40b2-922a-fdba0c0c5cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(2), np.float64(70.0), np.int64(257))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the minimum, median, and maximum text lengths in the train dataset\n",
    "text_len = train_dataset[\"text\"].apply(lambda x: len(x.split()))\n",
    "text_len.min(), text_len.median(), text_len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3cc32ea-ed00-4d1a-bd0e-c75fa759eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training dataset randomly\n",
    "train_dataset_shuffled = train_dataset.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Define validation set size as 20% of the shuffled training dataset\n",
    "valid_size = int(len(train_dataset_shuffled) * 0.2)\n",
    "\n",
    "# Split the train (80%) and validation (20%) datasets\n",
    "valid_dataset = train_dataset_shuffled.iloc[:valid_size]\n",
    "train_dataset = train_dataset_shuffled.iloc[valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "450d571d-da86-4941-be79-26359f8be8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2880000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 720000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 400000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create a Hugging Face DatasetDict to organize the data into train/validation/test splits\n",
    "amazon_dataset = DatasetDict({\n",
    "    \"train\" : Dataset.from_pandas(train_dataset),\n",
    "    \"validation\" : Dataset.from_pandas(valid_dataset),\n",
    "    \"test\" : Dataset.from_pandas(test_dataset)\n",
    "})\n",
    "\n",
    "print(amazon_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5da95653-491a-43a6-8d14-bb9774f25566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for the pretrained DistilBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1943df58-bb45-44df-84a5-9f79f500e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to tokenize text data for model input\n",
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3660673b-4925-4224-ac50-6d20eb942541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c7a43c754c4f6eb2ef5315cc033403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2880000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247548a68e9e4fa189a0d1d4a56b6b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/720000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6fee21521445f18ab746742f5d2f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the tokenize_text function to the entire DatasetDict\n",
    "amazon_tokenized = amazon_dataset.map(tokenize_text, batched=True, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ae918be-a970-4f44-a119-7a076e7c502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7adba99-f78c-43cb-a6e7-77cd6b446556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch Dataset\n",
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48a931cd-c304-45e8-ae00-1a1089bdfe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the tokenized DatasetDict into PyTorch-compatible Dataset objects\n",
    "train_dataset = AmazonDataset(amazon_tokenized, partition_key=\"train\")\n",
    "val_dataset = AmazonDataset(amazon_tokenized, partition_key=\"validation\")\n",
    "test_dataset = AmazonDataset(amazon_tokenized, partition_key=\"test\")\n",
    "\n",
    "\n",
    "# Create DataLoaders for the training, validation and test sets\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e0bdc85-39ac-42bf-a142-ad3ecb3cf378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained DistilBERT model for sequence classification\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac7355b0-0ace-4d35-bc92-463d0710a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters of the BERT model\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87b10b59-7b4a-4553-9587-d36ee7ae3292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67fc0930-a185-44b3-b087-c0278261b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the parameters of the pre-classifier layer\n",
    "for param in bert_model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the parameters of the classification head\n",
    "for param in bert_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4888d1bf-0d05-4261-8d35-0124dc0d323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch Lightning module to wrap the BERT classifier\n",
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate=learning_rate\n",
    "        self.model = model\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "        self.train_acc = Accuracy(task = \"multiclass\", num_classes=2)\n",
    "        self.val_acc = Accuracy(task = \"multiclass\", num_classes=2)\n",
    "        self.test_acc = Accuracy(task = \"multiclass\", num_classes=2)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n",
    "                       labels=batch[\"label\"])\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        return outputs, predicted_labels\n",
    "    \n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, predicted_labels = self._shared_step(batch, batch_idx)\n",
    "        \n",
    "\n",
    "        self.log(\"train_loss\", outputs['loss'])\n",
    "\n",
    "        self.train_acc(predicted_labels, batch[\"label\"])\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return outputs[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs, predicted_labels = self._shared_step(batch, batch_idx)\n",
    "\n",
    "        self.log(\"val_loss\", outputs[\"loss\"], prog_bar=True)\n",
    "\n",
    "\n",
    "        self.val_acc(predicted_labels, batch[\"label\"])\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs, predicted_labels = self._shared_step(batch, batch_idx)\n",
    "        \n",
    "        self.test_acc(predicted_labels, batch[\"label\"])\n",
    "        self.log(\"test_acc\", self.test_acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "24405a8c-c714-44f0-a6c4-cf2d98f3e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LightningModel with the pretrained BERT and a learning rate\n",
    "lightning_model = LightningModel(model=bert_model, learning_rate=0.01)\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_acc\")] # to save the best model\n",
    "\n",
    "# Set up a CSV logger to save training logs\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"bert-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3226329-df5d-4c59-8822-ef25315f8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a77bcd2c-05a2-40f0-9ed9-71a383deef72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 123\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | model     | DistilBertForSequenceClassification | 67.0 M | eval \n",
      "1 | train_acc | MulticlassAccuracy                  | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy                  | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy                  | 0      | train\n",
      "--------------------------------------------------------------------------\n",
      "592 K     Trainable params\n",
      "66.4 M    Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "96        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd334defc75417482cee5dd02ab0210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dae690bd0cc4cf3bc6e6b203e1ac924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328820ddba3f43558b079f9ca10fc926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2277439d48741b592f92998ed2ddb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa4aab0480c41929c8e4a3530c97638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "L.seed_everything(123)\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator='gpu',\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(model=lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d735e571-5fb1-42eb-9f4a-d8271b356ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/bert-model\\version_5\\checkpoints\\epoch=2-step=270000.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/bert-model\\version_5\\checkpoints\\epoch=2-step=270000.ckpt\n",
      "C:\\Users\\pc\\anaconda3\\envs\\new2\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb1c25148a648f6a8c1c613ba170b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9036549925804138     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9036549925804138    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.9036549925804138}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582f505-eb2c-4794-ad28-fb59615a4f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
